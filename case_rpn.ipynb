{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27f4fda-8a60-4a85-acfd-43779b80c763",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2efcc25-cd00-4202-9364-a3d9c2beb669",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#configs = {\n",
    "#  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "#  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
    "#}\n",
    "\n",
    "# Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "#dbutils.fs.mount(\n",
    "#  source = \"abfss://bronze-parquet-files@weiridatalake.dfs.core.windows.net/\",\n",
    "#  mount_point = \"/mnt/bronze\",\n",
    "#  extra_configs = configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ab14d1-302d-422c-94ac-2a90a40cacbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/bronze/customers.parquet', name='customers.parquet', size=14877, modificationTime=1723753599000),\n",
       " FileInfo(path='dbfs:/mnt/bronze/employees.parquet', name='employees.parquet', size=2711, modificationTime=1723753600000),\n",
       " FileInfo(path='dbfs:/mnt/bronze/offices.parquet', name='offices.parquet', size=2362, modificationTime=1723753600000),\n",
       " FileInfo(path='dbfs:/mnt/bronze/orderdetails.parquet', name='orderdetails.parquet', size=41567, modificationTime=1723753599000),\n",
       " FileInfo(path='dbfs:/mnt/bronze/orders.parquet', name='orders.parquet', size=9837, modificationTime=1723753599000),\n",
       " FileInfo(path='dbfs:/mnt/bronze/payments.parquet', name='payments.parquet', size=8355, modificationTime=1723753599000),\n",
       " FileInfo(path='dbfs:/mnt/bronze/product_lines.parquet', name='product_lines.parquet', size=4023, modificationTime=1723753599000),\n",
       " FileInfo(path='dbfs:/mnt/bronze/products.parquet', name='products.parquet', size=17152, modificationTime=1723753600000)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9caa499-fbec-42b0-9a23-43e05dc437e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482050-2283703241\", line 13, in bronze_customers\n    return (spark.read.format(\"parquet\").load(parquet_customers_path))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 47, in wrapper\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 312, in load\n    return self._df(self._jreader.load(path))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 254, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/bronze/customers/*.parquet. SQLSTATE: 42K03\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482050-2283703241\", line 13, in bronze_customers\n    return (spark.read.format(\"parquet\").load(parquet_customers_path))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 47, in wrapper\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 312, in load\n    return self._df(self._jreader.load(path))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 254, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/bronze/customers/*.parquet. SQLSTATE: 42K03\n\n"
     ]
    }
   ],
   "source": [
    "#TABELAS BRONZA PARA DELTA\n",
    "\n",
    "#customers\n",
    "\n",
    "parquet_customers_path = \"dbfs:/mnt/bronze/customers/*.parquet\"\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "comment= 'raw parquet from data lake',\n",
    "    table_properties={\"quality\": \"silver\"})\n",
    "def bronze_customers():\n",
    "\n",
    "    return (spark.read.format(\"parquet\").load(parquet_customers_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e72a9108-f9d4-4ce9-a2c7-122fa74a6955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482051-4146287839\", line 9, in bronze_orders\n    return (spark.read.format(\"parquet\").load(parquet_orders_path))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 47, in wrapper\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 312, in load\n    return self._df(self._jreader.load(path))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 254, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/bronze/orders/*.parquet. SQLSTATE: 42K03\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482051-4146287839\", line 9, in bronze_orders\n    return (spark.read.format(\"parquet\").load(parquet_orders_path))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 47, in wrapper\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 312, in load\n    return self._df(self._jreader.load(path))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 254, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/bronze/orders/*.parquet. SQLSTATE: 42K03\n\n"
     ]
    }
   ],
   "source": [
    "#orders\n",
    "\n",
    "parquet_orders_path = \"dbfs:/mnt/bronze/orders/*.parquet\"\n",
    "\n",
    "@dlt.table(\n",
    "comment= 'raw parquet from data lake',\n",
    "    table_properties={\"quality\": \"silver\"})\n",
    "def bronze_orders():\n",
    "    return (spark.read.format(\"parquet\").load(parquet_orders_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead16bd1-04dc-4665-a5f2-662879e4f224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482053-833398461\", line 7, in customers_enriched\n    dlt.read(\"bronze_customers\")\n  File \"/databricks/spark/python/dlt/api.py\", line 605, in read\n    pipeline.instance.get_scala_pipeline().read(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 248, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o3595.read.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'bronze_customers'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:9)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:545)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:452)\n\tat com.databricks.pipelines.Pipeline.read(Pipeline.scala:702)\n\tat sun.reflect.GeneratedMethodAccessor472.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy149.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:841)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1860)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1751)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1858)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:169)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:321)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1858)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1857)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:403)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:319)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:317)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:301)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:396)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:144)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:130)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:502)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:220)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:219)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:243)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:243)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:502)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:392)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:389)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:444)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:440)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:610)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:591)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:289)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:115)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:239)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:317)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:313)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:701)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.GeneratedMethodAccessor456.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482053-833398461\", line 7, in customers_enriched\n    dlt.read(\"bronze_customers\")\n  File \"/databricks/spark/python/dlt/api.py\", line 605, in read\n    pipeline.instance.get_scala_pipeline().read(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 248, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o3595.read.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'bronze_customers'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:9)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:545)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:452)\n\tat com.databricks.pipelines.Pipeline.read(Pipeline.scala:702)\n\tat sun.reflect.GeneratedMethodAccessor472.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy149.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:841)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1860)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1751)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1858)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:169)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:321)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1858)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1857)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:403)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:319)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:317)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:301)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:396)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:144)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:130)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:502)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:220)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:219)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:243)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:243)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:502)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:392)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:389)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:444)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:440)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:610)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:591)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:289)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:115)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:239)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:317)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:313)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:701)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.GeneratedMethodAccessor456.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMAÇÕES COM VIEWS - CAMADA SILVER\n",
    "\n",
    "\n",
    "@dlt.view(comment=\"sanitize data.\")\n",
    "def customers_enriched():\n",
    "    return (\n",
    "    dlt.read(\"bronze_customers\")\n",
    "    .select(\n",
    "        col(\"customer_number\").alias(\"user_id\"),\n",
    "        col(\"country\").alias(\"country_name\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3faa9c3-544b-4283-9744-7d50e5966f69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482054-2719045694\", line 4, in orders_enriched\n    dlt.read(\"bronze_orders\")\n  File \"/databricks/spark/python/dlt/api.py\", line 605, in read\n    pipeline.instance.get_scala_pipeline().read(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 248, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o3623.read.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'bronze_orders'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:9)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:545)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:452)\n\tat com.databricks.pipelines.Pipeline.read(Pipeline.scala:702)\n\tat sun.reflect.GeneratedMethodAccessor472.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy149.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:841)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1860)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1751)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1858)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:169)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:321)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1858)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1857)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:403)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:319)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:317)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:301)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:396)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:144)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:130)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:502)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:220)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:219)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:243)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:243)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:502)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:392)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:389)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:444)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:440)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:610)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:591)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:289)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:115)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:239)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:317)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:313)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:701)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.GeneratedMethodAccessor456.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482054-2719045694\", line 4, in orders_enriched\n    dlt.read(\"bronze_orders\")\n  File \"/databricks/spark/python/dlt/api.py\", line 605, in read\n    pipeline.instance.get_scala_pipeline().read(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 248, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o3623.read.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'bronze_orders'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:9)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:545)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:452)\n\tat com.databricks.pipelines.Pipeline.read(Pipeline.scala:702)\n\tat sun.reflect.GeneratedMethodAccessor472.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy149.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:841)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1860)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1751)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1858)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:169)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:321)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1858)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1857)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:403)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:319)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:317)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:301)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:396)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:144)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:130)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:502)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:220)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:219)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:243)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:243)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:502)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:392)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:389)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:444)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:440)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:610)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:591)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:289)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:115)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:239)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:317)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:313)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:701)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.GeneratedMethodAccessor456.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n"
     ]
    }
   ],
   "source": [
    "@dlt.view(comment=\"sanitize data.\")\n",
    "def orders_enriched():\n",
    "    return (\n",
    "    dlt.read(\"bronze_orders\")\n",
    "    .select(\n",
    "        col(\"order_number\").alias(\"order_id\"),\n",
    "        col(\"customer_number\").alias(\"order_user_id\"),\n",
    "        col(\"shipped_date\"),\n",
    "        col(\"order_date\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e3bd633-bdcf-4dd1-87c9-9e7f53aa63b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482063-2888877845\", line 8, in gold_country_orders\n    bronze_customers = dlt.read(\"customers_enriched\")\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 605, in read\n    pipeline.instance.get_scala_pipeline().read(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 248, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o3651.read.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'customers_enriched'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:9)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:545)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:452)\n\tat com.databricks.pipelines.Pipeline.read(Pipeline.scala:702)\n\tat sun.reflect.GeneratedMethodAccessor472.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy149.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:841)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1860)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1751)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1858)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:169)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:321)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1858)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1857)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:403)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:319)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:317)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:301)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:396)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:144)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:130)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:502)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:220)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:219)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:243)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:243)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:502)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:392)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:389)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:444)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:440)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:610)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:591)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:289)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:115)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:239)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:317)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:313)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:701)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.GeneratedMethodAccessor456.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n          ^^^^^^^^^^^\n  File \"/root/.ipykernel/1098/command-400138607482063-2888877845\", line 8, in gold_country_orders\n    bronze_customers = dlt.read(\"customers_enriched\")\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/dlt/api.py\", line 605, in read\n    pipeline.instance.get_scala_pipeline().read(name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 248, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o3651.read.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'customers_enriched'. Dataset is not defined in the pipeline.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:9)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:545)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:452)\n\tat com.databricks.pipelines.Pipeline.read(Pipeline.scala:702)\n\tat sun.reflect.GeneratedMethodAccessor472.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:261)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy149.call(Unknown Source)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:841)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:1860)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:294)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:292)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:1751)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:1858)\n\tat com.databricks.pipelines.DefaultPipelineContextStack.withContext(Pipeline.scala:169)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:321)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$2(Pipeline.scala:1858)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:1857)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:403)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:319)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:317)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:301)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:396)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:144)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:130)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult$lzycompute(Flow.scala:502)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:220)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:219)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:502)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:243)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:243)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:502)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$attemptResolveFlow$1(DataflowGraph.scala:392)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.attemptResolveFlow(DataflowGraph.scala:389)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolveSerially$1(DataflowGraph.scala:444)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolveSerially(DataflowGraph.scala:440)\n\tat com.databricks.pipelines.graph.DataflowGraph.$anonfun$resolve$1(DataflowGraph.scala:610)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:532)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:654)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:253)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:249)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:541)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:533)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:501)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.DataflowGraph.resolve(DataflowGraph.scala:591)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.$anonfun$analyzeWithSparkConfOverrides$1(AnalysisHandler.scala:289)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSQLConf(SparkSessionUtils.scala:19)\n\tat com.databricks.pipelines.util.SparkSessionUtils$.withSparkSession(SparkSessionUtils.scala:88)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.withAdditionalSparkConfs(AnalysisHandler.scala:115)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyzeWithSparkConfOverrides(AnalysisHandler.scala:239)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze(AnalysisHandler.scala:317)\n\tat com.databricks.pipelines.execution.core.AnalysisHandler.analyze$(AnalysisHandler.scala:313)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate$.analyze(LocalMesaEngine.scala:701)\n\tat com.databricks.pipelines.execution.core.SynchronousUpdate.analyze(LocalMesaEngine.scala)\n\tat sun.reflect.GeneratedMethodAccessor456.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n"
     ]
    }
   ],
   "source": [
    "# CRIAÇÃO DAS TABELAS GOLDS\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Country orders cleaned table\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_country_orders():\n",
    "    bronze_customers = dlt.read(\"customers_enriched\")\n",
    "    bronze_orders = dlt.read(\"orders_enriched\")\n",
    "\n",
    "    # Join datasets using an inner join\n",
    "    join_country_orders = bronze_orders.join(\n",
    "        bronze_customers,\n",
    "        bronze_orders.order_user_id == bronze_customers.user_id,\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    result_df = join_country_orders.select(\n",
    "        \"order_id\",\n",
    "        \"order_user_id\",\n",
    "        \"shipped_date\",\n",
    "        \"order_date\",\n",
    "        \"user_id\",\n",
    "        \"country_name\"\n",
    "    )\n",
    "\n",
    "    return final_query_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 400138607482061,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "case_rpn",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
